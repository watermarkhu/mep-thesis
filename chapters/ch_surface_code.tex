\chapter{The surface code}\label{ch:surfacecode}

The variant of the stabilizer codes that we are going to explore in this thesis is Kitaev's \emph{surface code} \cite{kitaev2003fault}, which is of the category of \emph{topological codes}. Among this category, the surface code is preferable as it offers the highest error tolerance under realize noise channels and requires only local stabilizer measurements of physically neighboring qubits. We consider two variants of the surface code in this thesis, the \emph{toric code} in Section \ref{sec:surface_toric} and the \emph{planar code} in Section \ref{sec:surface_planar}.
\input{tikzfigs/toric_lattice}

\section{The toric code}\label{sec:surface_toric}
The \emph{toric code} is defined by arranging qubits on the edges of a square lattice with periodic boundary conditions, as seen in Figure \ref{sf:fig_toriclattice}. Its name is inspired by the torus, where any point on the torus's surface will encounter itself after traversing the torus in either x or y directions. Hence, the top edge of the toric code meets the bottom edge, whereas the left edge meets the right. On a $L\times L$ grid, there are $n = 2L^2$ edges and the same amount of physical qubits. This topology of qubit arrangement plays an integral part in encoding the logical qubits, which are stored in the torus' non-trivial cycles. Errors, below a certain threshold, will only introduce local effects and do not change these cycles.

\subsection{Stabilizer generators}

To define a stabilizer code, we need to specify the $m$ independent stabilizer generators and the encoded $\hat{X}_L$ and $\hat{Z}_L$ operators. On the toric code, there are two types of stabilizer generators, \emph{plaquette} and \emph{star} operators, which are associated with the \emph{faces} and \emph{vertices} of the square lattice.
\input{tikzfigs/toric_stabilizers}

\begin{definition}
  A plaquette operator $\hat{P}_f$ consists of the tensor product of Pauli Z operators on qubits on the edges connected to a face $f$ (see Figure \ref{sf:fig_toriclattice}a),
  \begin{equation}\label{eq:sf_plaquette}
    \gls{oplaquette} = \prod_{i\in Q(f)} \hat{Z}_i
  \end{equation}
  where $Q(f)$ is the set of qubits touching face $f$. On a $L\times L$ grid, there are $L^2$ plaquettes.
\end{definition}

\begin{definition}
  A star operator $\hat{S}_v$ consists of the tensor product of Pauli X operators on qubits neighboring the vertex $v$ (see Figure \ref{sf:fig_toriclattice}b),
  \begin{equation}\label{eq:sf_star}
    \gls{ostar} = \prod_{i\in Q(v)} \hat{X}_i
  \end{equation}
  where $Q(v)$ is the set of qubits neighboring vertex $v$. On a $L\times L$ grid, there are $L^2$ plaquettes.
\end{definition}

As each plaquette and star operator needs to be measured, an ancilla qubit is required at the physical locations of each of these operators. The structure of the full lattice is now clear, as it just a simple square arrangement of alternating data and ancilla qubits in both x and y directions.

The full stabilizer of the code $\m{S}$ can be generated by multiplying elements of the generator operators (see Definition \ref{def:stabilizer}). Consider two plaquette operators. These two operators will either share one boundary consisting of a qubit, or none. This effect means that the Pauli Z operator on the boundary qubit will add up to identity as they commute. The result is that the product of the plaquette operators will consist of the overall boundary Pauli operators of the joint plaquette (see Figure \ref{sf:fig_multistab}a).

However, if all plaquettes are applied to the lattice, no boundary will be left. Thus, the product of all plaquettes is the identity, which means that the full set of plaquettes are not independent. Therefore, the full set of plaquette generators can be completed by simply removing a single plaquette from all available plaquettes. There are consequently $L^2 - 1$ independent plaquette operators.

The multiplication of star operators follow the same properties as the plaquette operators described above (see Figure \ref{sf:fig_multistab}b). Thus, there are also $L^2 - 1$ independent star operators, which are the star generators. Consequently, there are $n_s = 2L^2 - 2$ independent stabilizer generators.
\input{tikzfigs/toric_multiplication}

\subsection{Dual lattice}
Note that if we shift our lattice half a cell down, and half a cell to the right, we can create a \emph{dual} lattice. This dual lattice has the same size and same boundary conditions as the \emph{primal} lattice, but every plaquette in the primal lattice is a star in the dual lattice, and every star in the primal lattice is a plaquette in the dual lattice. In Figure \ref{sf:fig_toriclattice}, the edges of the dual lattice are plotted with dotted lines.

This exciting property of \emph{lattice duality} leads to the fact that plaquette and star operators are, in fact, the same, and we can choose from either based on which is best suited for the calculation. The multiplication of operators is best pictured in the plaquette picture, for example. For the square lattice in the toric code, the dual lattice is coincidentally also square. For other types of topological codes with non-square lattices, the dual lattice has a different lattice structure than the primal lattice. We will not explore these kinds of lattices in this thesis.

\subsection{Encoded qubits}
Since there are $n = L^2$ qubits and $n_s = 2L^2 - 2$ independent stabilizers, we must have $n_l = n - n_s = 2$ encoded qubits and, therefore, four logical operators $\hat{X}_{L1}, \hat{X}_{L2}, \hat{Z}_{L1}$ and $\hat{Z}_{L2}$.

Recall the logical operators consists of the Pauli operators, and must commute with all stabilizer generators, but cannot be part of the stabilizer itself. We can construct the logical operators by starting with, for example, a single Pauli Z operator. It commutes with all plaquette operators trivially. In terms of the star operators, this single Pauli Z operator commutes with all but the two neighboring qubits, as all others apply to different qubits. Adding another Pauli Z operator will shift will of the neighboring star operators that anticommute. We see that a closed loop of Z operators around the torus does not have neighboring star operators, and therefore commute with all stabilizers. As the torus has two directions we can loop over, these are the logical $\hat{Z}_L$ operators (see Figure \ref{sf:fig_logical}a-b). Analogously, we can construct the logical $\hat{X}_L$ operators in the same way (Figure \ref{sf:fig_logical}c-d).

Note that these logical operators are not unique. As the logical operators commute with the stabilizers, these $\hat{X}_L$ and $\hat{Z}_L$ operators can be multiplied with, for example, a plaquette or star operator, respectively, that creates a diversion from its original path. But as the path still loops around the torus, this is still a valid logical operator.

The logical operators have a minimum length of $L$ qubits, which is also the distance of the toric code (see Definition \ref{def:distance}). Thus, the toric code is a $[L^2,2,L]$ code in the $[n,k,d]$ notation. This implies that the toric code might be more robust against errors if the size of the lattice is increased. Later we will see that this is also very much dependent on the type of decoder used and that different decoders will lead to different regimes of error for which this reasoning is valid.
\input{tikzfigs/toric_logical}

\subsection{Error detection}
As discussed in the previous chapter, errors are detected by measuring the set of stabilizer generators. As we have seen in the last section, this set consists of all but a single plaquette and star operator. Let us first consider measuring all of them.

In the case of a single Z-error (Fig \ref{sf:fig_degenerate}a.i), the neighboring plaquette operators will commute with this error, as it consists of Pauli Z operators itself. But the neighboring star operators anticommutes with this error according to equation \eqref{qec:eq:stabmeas}. Similarly, a single X-error (Fig \ref{sf:fig_degenerate}a.ii) commutes with neighboring star operators but anticommutes with neighboring plaquette operators. A Y-error is a combination of X and Z operators and, therefore, anticommutes with all neighboring generator operators (Fig \ref{sf:fig_degenerate}a.iii).

In the case of two Z-errors (Fig \ref{sf:fig_degenerate}a.iv), the star operators between the two errors now commute with the errors, creating a virtual path between them. This effect of commutation is a general property: given any chain of errors, the generator operators at the end of the chain will anticommute with the errors and measure -1. For Z-errors, star operators at the end of chains on the primal lattice will measure -1. The detection of X-errors occurs in the same way, albeit now the chains of errors are defined on the \emph{dual} lattice, and plaquette errors will measure -1 at the end of these chains.

Since Z and X-errors independently affect different types of stabilizer measurements (stars and plaquettes, respectively), these two types of errors can be considered separately in two error correction processes. The two processes are analogous, up to the duality of the lattice. Therefore, for the remainder of the section, only Z-errors, which leave a string of errors on the primal lattice, will be considered.
\input{tikzfigs/toric_classes}

\subsection{Error correction}\label{sec:correction}
An error can be corrected by applying it again to the lattice. However, the problem is that the error operator $\hat{E}$ is unknown. For this reason, we must try to identify the correct operator given the measured syndrome. As mentioned in the previous chapter, this relationship between error does not always map one-to-one. This is also the case in the surface code. An error $\hat{E}$ can be multiplied with some operator $L$ that commutes with the stabilizer, resulting in the same syndrome.

If $\hat{L}$ is in the stabilizer $\m{S}$, the product of the identified correction operator $\gls{ocorrection}=\hat{E}'$ with the real error operator $\hat{E}$ will leave the code invariant. The resulting operator, $\hat{C}\hat{E}\hat{L}$, is a stabilizer operator $\hat{S}$. However, the encoded logical operators also commute with the stabilizer, which means that $\hat{E}$, $\hat{E}\hat{Z}_{L1}$, $\hat{E}\hat{Z}_{L2}$, $\hat{E}\hat{Z}_{L1}\hat{Z}_{L2}$ will all lead to the same syndrome (Fig \ref{sf:fig_degenerate}a-d). Any identified correction operator $\hat{C}$ can be categorized into four classes of operators, of which only one includes the correct logical operator. The task of choosing the most appropriate correction chain is up to the decoders.

\subsection{Decoding}\label{sec:threshold}
The distance $d$ of the toric code on a $L\times L$ is $L$. For this reason we expect that we can improve the robustness of the code by increasing the lattice size $L$. However, this also increases the total number of errors in the lattice, which adds an increased level of complexity in choosing the correct correction operator.

\begin{definition}\label{def:pthres}
  Let the error threshold $\gls{zpthres}$ be the transition point in the error rate below which increasing the lattice size of the surface code will increase the success rate of decoding $\gls{zpcorrect}$. Let the success rate of decoding at the transition point be $k_{th}$. 
\end{definition}

In practice, there is a trade-off between the positive effect of an increased code distance and the negative effect of a larger number of errors. When the error rate $p$ is low, the positive effect outweighs the negative, and increasing the lattice $L$ will increase the probability of successful error correction $\gls{zpcorrect}$. When the error rate is large, the negative effect outweighs the positive, and increasing $p$ will decrease $k_C$. 

The code threshold is not the only parameter that determines the potential of a particular code for practical use. The behavior for error rates far below the threshold is also essential, as is the number of physical qubits needed to achieve the sought after level of error suppression. Nevertheless, the code threshold provides a straightforward and useful tool to benchmark different codes and decoding algorithms and to compare them with each other. It is common to benchmark codes under the independent and phenomenological noise models of \ref{qec:sec_errormodels}. For this reason, in this thesis, we will heavily rely on the value of the code threshold. 

For the toric code, when the only source of errors is from the independent noise model, the \emph{optimal threshold} has been proven to be $p_{th}=0.1094$, and including measurement errors $p_{th} = 0.0373$. These values are obtained by matching the pairing problem to the two-dimensional random-bond Ising model \cite{dennis2002topological}. The error threshold is the critical point at which the Nishimori line crosses the boundary between the magnetic and the paramagnetic phase. An \emph{optimal} decoder that achieves this value requires considering all possible error configurations on the lattice to identify the correction operator $\hat{C}$ that is most likely to be equal to the error operator $\hat{E}$. This is a computationally heavy task that scales exponentially with the lattice size and is an impractical approach in reality.

Luckily, there exist other decoding algorithms that can find a solution much faster, albeit at the cost of reducing the code threshold. When surface codes were introduced \cite{dennis2002topological}, a decoder was suggested that scales cubic with the system, which allows for faster decoding. The Minimum-Weight Perfect Matching decoder, based on Edmond's Blossom algorithm achieves a code threshold of 10.3\% (Chapter \ref{ch:mwpm}). Including faulty measurements, the threshold drops to 2.9\%. In recent years, the development of decoders for the surface code has in majority favored the class of \emph{neural network} decoders, in which a particular model is trained to give better predictions on the correction operator \cite{breuckmann2018scalable, nautrup2019optimizing, varsamopoulos2017decoding, varsamopoulos2020decoding}. 

A promising and fast decoder is the \emph{Union-Find} decoder, which is a relatively new addition to the realm of surface code decoders \cite{delfosse2017almost}. It scales \emph{almost} linearly with the system and has a code threshold of 9.9\% (Chapter \ref{ch:UFdecoder}). The former is a significant advantage of the Union-Find decoder as the scalability translates directly into the computation time needed to correct. This allows the increase of the system size to obtain enhanced fault-tolerance without dramatically increased computation times. The Minimum-Weight Perfect Matching decoder neural network decoders and other types of decoder all suffer in this regard. In fact, there is no decoder currently available that is fast enough to correct the accumulation of errors with the current physical limitations. Another benefit of the Union-Find decoder over other types of decoders is because it is \emph{simple}. Even though it may not seem so due to the length of this thesis, the concept of the Union-Find decoder is much more straightforward compared with other, more advanced decoders. 

\subsection{Quasiparticle picture}\label{sec:quasiparticle}
The error detection and correction processes can alternatively be presented in the \emph{quasiparticle picture}. The anticommuting stabilizer measurements now act like excitations on the lattice, which behave like the quasiparticles \emph{anyons}. A single error creates a pair of anyons, and a chain of errors causes movement of the anyon on the lattice. A pair of anyons can also annihilate each other when two error chains merge. The correction of errors can thus be viewed of movement of the correction chains until all anyons are annihilated. The quasiparticle picture removes the distracting underlying lattice from the problem, and decoding becomes merely identifying the right pairing between anyons to minimize the chance of a logical error.
\input{tikzfigs/quasiparticle_picture}

Figure \ref{fig:quasiparticle}a shows the quasiparticle representation of the errors suffered in Figure \ref{sf:fig_degenerate}a, which has suffered Z (blue lines) and X errors (red lines). The corresponding anyons can either be of the star type (red circle) or plaquette type (blue circle). Figure \ref{fig:quasiparticle}b shows a successful decoding. Note that not all pairs are correctly identified, but the resulting loop still is in the same class of operators. In Figure \ref{fig:quasiparticle}c, the correction has failed as the resulting loop in the correction is in a different class compared to the error. As the loop still commutes with the stabilizer, no error can be detected, but the encoded qubit has acquired a logical error.


\subsection{Graph picture}\label{sec:toricgraph}
A surface code can also be denoted by a graph $\gls{ngraph}$. The graph is constructed by $G(\gls{svertices}, \gls{sedges}, \gls{sfaces})$ by a vertex set $\m{V}$, edge set $\m{E}$, and face set $\m{A}$. Each edge $\gls{nedge}\in \m{E}$ is defined by a pair of distinct vertices $e=\{v_i, v_j\}$ where $\gls{nvertex}\in \m{V}$. Each face is a region that has the homology of a disk and is defined by the set of edges on its boundary. 

Concerning the toric code, each vertex refers to an ancillary qubit corresponding to a star operator $v\equiv \hat{S}_v$. Each face $\gls{nface}\in \m{A}$ refers to an ancillary qubit corresponding to a plaquette operator $f\equiv \hat{P}_f$. The edges are thus the qubits, with two edges per qubit, one spanned by neighboring vertices and another by neighboring faces. Due to the duality of the lattice, the equivalence of vertices with stars and faces with plaquettes can naturally be switched. Also, the graph $G$ can be split into two separate graphs $G_v(\m{V}, \m{E}_v)$ and $\m{G}_v(\m{A}, \m{E}_v)$, corresponding to the primal and dual lattices, where $E_v$ are the edges connecting the vertices $\m{V}$, and $\m{E}_f$ are the edges connecting the faces $\m{A}$. Each qubit is now represented by a single edge in both graphs and $\m{E}_v\cup \m{E}_f=\m{E}$. If we mention a graph spanned by only vertices and edges $(\m{V},\m{E})$, we refer to the graph $G_v$ of the primal lattice. 


\section{The planar code}\label{sec:surface_planar}
Another variant of the surface code is the \emph{planar code} that disposes of the torus's periodic boundary conditions. This allows the qubits to be placed onto a physical flat two-dimensional surface. For this reason, the planar code more accurately represent real systems in which the qubits physically interact with each other. Therefore, in this thesis, we will consider both toric and planar variants of the surface code.
\input{tikzfigs/planarcode}

\subsection{Stabilizer generators}
There are a few key differences between the planar and toric codes. First, a new type of stabilizer generators defines the lattice's non-periodic boundary, which is referred to as \emph{$\delta$ operators}. These $\delta$ operators have only three neighboring qubits, and are the tensor product of three Pauli operators. The $\delta$-plaquette operators lie at the lattice's east and west boundaries (Figure \ref{sf:fig_planar}a) and the $\delta$-star operators lie at the lattice's north and south boundaries (Figure \ref{sf:fig_planar}b). In the middle of the lattice, the bulk of the stabilizer generators still consist of 4 Pauli operators, identical to the ones in the toric code (Figure \ref{sf:fig_planar}c-d). Note that the stabilizer generators are always defined by equation \eqref{eq:sf_plaquette} and \eqref{eq:sf_star}, but now the relevant faces and vertices contain three neighboring qubits.

\subsection{Stabilizer violations}
A second key difference is that now not all errors will cause two stabilizer violations. A single error will still cause two neighboring stabilizers to measure -1 and create two anyons in the bulk of the qubits on the lattice. At the boundary, however, it now may be the case that an error is only included in one plaquette or star operator. This will also mean the decoding in the quasiparticle picture requires a slightly different approach.

\subsection{Encoded qubits}
Furthermore, we can inspect that a planar surface of dimension $L$ has $n = 2L^2-2L+1$ physical qubits. We can also find that there are $2L^2-2L$ stabilizer generators. As the boundary is now non-periodic, all generators are now independent. Therefore, the number of independent generators is $n_s = 2L^2-2L$. This means that the planar code encodes $n_l = n-n_s = 1$ a single logical qubit. The logical $\hat{X}_L$ and $\hat{Z}_L$ operators are depicted in Figure \ref{sf:fig_planar}e-f.

\subsection{Dual lattice and graph picture}
Other properties of the planar code are very similar to the toric code. The \emph{dual lattice} also exists for the planar code, for example. But the dual lattice exists at a 90-degree angle compared to the primal lattice to account for the location of the boundary. The elements of the graph $G(\m{V},\m{E},\m{A})$ can now be separated into disjoint subsets due to the different properties of its vertices, edges, and faces. The vertex set $\m{V}$ can now be divided into; the internal vertex set $\m{V}_\iota$ consisting of 4-Pauli-operator stabilizers, the boundary vertex set $\m{V}_{\delta}$ consisting of 3-Pauli-operator stabilizers, and the open vertex set $\m{V}_\omega$ consisting of non-stabilizer vertices that span edges on the boundary $\m{E}_{\delta}$. Note that vertices of $\m{V}_\omega$ thus do not correspond to any physical stabilizer measurements. Edges connected to two stabilizer vertices belong to the internal edge set $\m{E}_\iota$. The graph for the primal lattice is hence $G(\m{V}_\iota\cup \m{V}_\delta \cup \m{V}_\omega, \m{E}_\iota\cup \m{E}_\delta)$. Elements of the graph for the dual graph can be separated into disjoint subsets in the same way.

\vspace{1em}
The bulk of the planar code is similar to the toric code, where the stabilizer generators consist of 4 Pauli operators. This is especially true as the system size $n$ increases, as the internal elements scale with $n$ and the boundary elements scale with $\sqrt{n}$. Hence, the decoding algorithms for the planar code is very similar to the toric code, albeit some slight alterations will be needed. 

\section{Faulty measurements}\label{sec:faultymeasurments}

Under the phenomenological noise model \ref{def:pheno}, which includes faulty measurements, the surface code can not be decoded with just a single cycle of stabilizer measurements. As for any measured syndrome, we are unsure whether it is caused by a qubit or measurement error. This can be solved by making multiple rounds of stabilizer measurements where we gain information about these measurement errors. 

With multiple rounds of measurement, the set of stabilizer measurements is now a three-dimensional array in which the third dimension represents time. For a $L\times L$ lattice, it is common to perform $L$ rounds of measurements, which results in a $L \times L \times L$ array. The space domain of this array represents the measurements taken during a single time-step. In the quasiparticle representation, an anyon is now defined as a measurement that differs from its value in the previous round of measurement. A physical error on a qubit now causes a set of anyons separated in space within the same time-step. In contrast, a measurement error creates a pair of anyons separated in the array's time dimension. Decoding is now equivalent to paring anyons in a three-dimensional space. 

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.5\textwidth]{fig/3dlattice.pdf}
  \hspace{1cm}
  \begin{tikzpicture}[scale=0.7]
    \draw[black!30!white, line width=1] (0,3) circle (.3) {};
    \draw[black!30!white, line width=1, fill=mred] (0,2) circle (.3) {};
    \draw[black!30!white, line width=2] (-.3,1) -- (.3,1) (0,1.3) -- (0,0.7);
    \draw[Dandelion, line width=2] (-.3,0) -- (.3,0) (0,.3) -- (0,-.3);
    \node[star, star points=5, star point ratio=0.5, draw=Dandelion, fill=Dandelion] at (0,-1) {}; 
    \node[align=left, anchor=west] at (1,3) {qubit};
    \node[align=left, anchor=west] at (1,2) {X-error};
    \node[align=left, anchor=west] at (1,1) {stabilizer};
    \node[align=left, anchor=west] at (1,0) {syndrome};
    \node[align=left, anchor=west] at (1,-1) {anyon};
  \end{tikzpicture}
  \caption{When noisy measurements are included, it is required to make multiple rounds of stabilizer measurement, which forms the z-axis of the three-dimensional array, with the positive z-axis equivalence to a later time-step. The $4\times 4$ toric lattice is here represented as a $4\times 4\times 4$ array. The anyons are now defined as a syndrome that differs from its value in the previous time-step. Note that the anyons in the bottom left of the figure are not caused by an X-error. Image of the three-dimensional lattice is obtained via our Python3 implementation of surface code simulations (Appendix \ref{ap:oopsurfacecode}).}
\end{figure}

\section{Threshold simulations}\label{sec:simthres}

In the coming chapters, we describe two decoders and introduce a new decoder that combines elements from the previous two. To benchmark the performances of these decoders, we simulate for the decoder success rate $k_C$ on toric and planar lattices for the independent noise model (Definition \ref{def:independent}), and on lattices for the phenomenological noise model (Definition \ref{def:pheno}). Under these error models, the lattice can be separated into the primal lattice containing the star operator X-parity check measurement, and the dual lattice containing the plaquette operator or Z-parity check measurements. Any simulation will be performed on the primal lattice with a single error rate $p_X$ for both bit-flips and noisy measurements. The dual lattice is under these conditions equivalent to the primal lattice. 

To find the code threshold $p_{th}$, we will perform Monte Carlo simulations of a lattice with some random error according to the input error model and the decoder process. The number of successful corrections divided by the number of simulations is a measure for the decoder success rate $k_C$. We will simulate for a range of lattice sizes and error rates around the suspected threshold value. We will then fit the acquired data to the function
\begin{align}\label{eq.4.fit}
% \nonumber % Remove numbering (before each equation)
  \nonumber k_{C} =& A + Bx + Cx^2 \\
    %+ \begin{cases}
    %   D_{even}\cdot L^{-1/\mu_{even}} &\text{L even}\\
    %   D_{odd}\cdot L^{-1/\mu_{odd}} &\text{L odd}
    % \end{cases}\\
  \text{with } x =& (p_X - p_{th})L^{1/\nu}
\end{align}
for $p_{th}$ \cite{wang2003confinement}. Here, all but $k_{C}$, $p$, and $L$ are fitting parameters. The threshold error rate $p_{th}$, the decoding success rate at the threshold $k_{th}$ and the running times will be used to benchmark and compare decoders. 

The Monte Carlo simulations are performed on our own implementation in Python3 (see Appendix \ref{ap:oopsurfacecode}). All thresholds are pre-computed on a 3.20Ghz Intel Core i5-4460 CPU with 16 Gigabytes of memory. The range of error rates is then refined around this threshold value and passed to a compute-cluster for rapid Monte Carlo simulations. The compute-cluster used in this thesis is the \emph{Cartesius} system belonging to the SURF cooperative in Amsterdam, The Netherlands, which has 3.60Ghz Intel Xeon E5 CPUs with 64 Gigabytes of memory. On every CPU, we maximize parallelization by running 24 threads of individual Monte Carlo simulations. 

% Note that there are distinct values for $D$ and $\mu$ for even and odd lattices. This is due to a discrepancy in the decoder threshold caused by a non-negligible finite-size effect for even and odd lattices. Therefore, for each fit done on any dataset, only data from even or odd lattices will be selected.