The surface code

The variant of the stabilizer codes that we are going to explore in this thesis is Kitaev's surface code [0], which is of the category of topological codes. Among this category, the surface code is preferable as it offers the highest error tolerance under realize noise channels and requires only local stabilizer measurements of physically neighboring qubits. Two variants of the surface code will be considered here, the toric code in Section X and the planar code in Section X.


The toric code
The toric code is defined by arranging qubits   the edges of a square lattice with periodic boundary conditions, as seen in Figure X. Its name is inspired by the torus, or doughnut, shape, where any point on the surface of the torus will encounter itself after traversing the torus in either x or y directions. Hence, the top edge of the toric code meets the bottom edge, whereas the left edge meets the right. On a X grid there are X edges and the same amount of physical qubits. This topology of qubit arrangement plays an important part in encoding the logical qubits, which is stored in the non-trivial cycles on the torus. Errors, beneath a certain threshold, will only introduce local effects and does not change these cycles.

Stabilizer generators

To define a stabilizer code, we need to specify the m independent stabilizer generators and the encoded X and X operators. On the toric code there are two types of stabilizer generators, plaquette and star operators, which are associated with the faces and vertices of the square lattice, respectively.


definition
  A plaquette operator X consists of the tensor product of Pauli Z operators on qubits on the edges connected to a face f (see Figure Xa),
  where X is the set of qubits touching face f. On a X grid there are X plaquettes.
definition

definition
  A star operator X consists of the tensor product of Pauli X operators on qubits neighboring the vertex v (see Figure Xb),
  where X is the set of qubits neighboring vertex v. On a X grid there are X plaquettes.
definition

As each plaquette and star operator needs to be measured, an ancilla qubit is needed at the physical locations of each of these operators. The structure of the full lattice is now clear, as it just a simple square arrangement of alternating data and ancilla qubits in both x and y directions.

The full stabilizer of the code X can be generated by multiplying elements of the generator operators (see Definition X). Consider two plaquette operators. These two operators will either share one boundary consisting of a qubit, or none. This means that the Pauli Z operator on the boundary qubit will add up to identity as they commute. The result is that the product of the plaquette operators will consists of the overall boundary Pauli operators of the joint plaquette (see Figure Xa).

However, if all plaquettes are applied to the lattice, no boundary will be left. Thus, the product of all plaquettes is the identity, which means that the full set of plaquettes are not independent. The full set of plaquette generators can therefore be completed by simply removing a single plaquette from all available plaquettes. There are therefore X independent plaquette operators.

The multiplication of star operators follow the same properties as the plaquette operators described above (see Figure Xb). Thus, there are also X independent star operators, which are the star generators. This sums up to X independent stabilizer generators.


Dual lattice
Note that if we shift our lattice half a cell down, and half a cell to the right, we can create a dual lattice. This dual lattice has the same size and same boundary conditions as the primal lattice, but every plaquette in the primal lattice is a star in the dual lattice, and every star in the primal lattice is a plaquette in the dual lattice. The edges of the dual lattice are plotted with dotted lines in the figures.

This interesting property of lattice duality leads to the fact that plaquette and star operators are in fact the same, and we can choose from either that is best suited for the calculation. The multiplication of operators is best pictured in the plaquette picture, for example. For the square lattice in the toric code, the dual lattice is coincidentally also square. For other types of topological codes with non-square lattices, the dual lattice has a different lattice structure than the primal lattice. We will not explore these kinds of lattices in this thesis.

Encoded qubits
Since there are X qubits and X independent stabilizers, we must have X encoded qubits and therefore 4 logical operators X and X.

Recall the logical operators consists of the Pauli operators, and must commute with all stabilizer generators, but cannot be part of the stabilizer itself. We can construct the logical operators by starting with, for example, a single Pauli Z operator. It commutes with all plaquette operators trivially. In terms of the star operators, this single Pauli Z operator commutes with all but the two neighboring qubits, as all others apply to different qubits. Adding another Pauli Z operator will shift will of the neighboring star operators that anticommute. We know see that a closed loop of Z operators around the torus does not have neighboring star operators, and therefore commute with all stabilizers. As the torus has two directions we can loop over, these are the logical X operators (see Figure Xa-b). Analogously, we can construct the logical X operators in the same way (Figure Xc-d).

Note that these logical operators are not unique. As the logical operators commute with the stabilizers, these X and X operators can be multiplied with e.g.\ a plaquette or star operator, respectively, which create a diversion from its original path. But as the path still loops around the torus, this is still a valid logical operator.

The logical operators have a minimum length of L qubits, which is also the distance of the toric code (see Definition X). The toric code is therefore a X in the X notation. This implies that the toric code might be more robust against errors if the size of the lattice is increased. Later we will see that this is also very much dependent on the type of decoder that is used, and that different decoders will lead to different regimes of error for which this reasoning is true.


Error detection
As discussed in the previous chapter, errors are detected by measuring the set of stabilizer generators. As we have seen in the previous section, this consists of all but a single plaquette operator and all but a single star operator. Let us first consider measuring all of them.

In the case of a single Z-error (Fig Xa.i), the neighboring plaquette operators will commute with this error, as it consists of Pauli Z operators itself. But the neighboring star operators anticommutes with this error according to equation X. Similarly, a single X-error (Fig Xa.ii) commutes with neighboring star operators but anticommutes with neighboring plaquette operators. An Y-error is a combination of X and Z operators and therefore anticommutes with all neighboring generator operators (Fig Xa.iii).

In the case of two Z-errors (Fig Xa.iv), the star operators between the two errors now commute with the errors, creating a virtual path between them. This is a general property: given any string of errors, the generator operators at the end of the string will anticommute with the errors and measure -1. For Z-errors, star operators at the end of strings on the primal lattice will measure -1. The detection of X-errors occur in the same way, albeit now the strings of errors is defined on the dual lattice, and plaquette errors will measure -1 at the end of these strings.

Since Z and X-errors independently affect different types of stabilizer measurements (stars and plaquettes, respectively), these two types of errors can be considered independently in two error correction processes. The two processes are analogous, up to the duality of the lattice. Therefore, for the remainder of the section, only Z-errors, which leave a string of errors on the primal lattice, will be considered.


Error correction
An error can be corrected by applying it again to the lattice. However, the problem is that the error operator X is unknown. We must therefore try to identify the correct operator given the measured syndrome. As mentioned in the previous chapter, this relationship between error does not always map one-to-one, which it is not in the surface code. An error X can be multiplied with some operator L that commutes with the stabilizer, and they will result in the same syndrome.

If X is in the stabilizer X, the product of the identified correction operator X with the real error operator X will leave the code invariant. The resulting operator X is a stabilizer operator X. However, the encoded logical operators also commute with the stabilizer, which means that X, X, X, X will all lead to the same syndrome (Fig Xa-d). Any identified correction operator X can therefore be categorized into four classes of operators, of which only one includes the correct logical operator. The task of choosing most appropriate correction chain is up to the decoders.

Decoding
Since the distance d of the toric code on a X is L, we would expect that we can improve the robustness of the code by increasing the lattice size L. However, this also increases the total number of errors in the lattice, that adds an increased level of complexity in choosing the correct correction operator.

definition
  Let the error threshold X be the transition point in the error rate below which increasing the lattice size of the surface code will increase the success rate of decoding X. Let the success rate of decoding at the transition point be X. 
definition

In practice, there is a trade-off between the positive effect of a larger code distance and the negative effect of larger number of errors. When the error rate p is low, the positive effect outweighs the negative and increasing the lattice L will increase the probability of successful error correction X. When the error rate is large, the negative effect outweighs the positive and increasing p will decrease X. The code threshold is not the only parameter that determines the potential of a certain code for practical use. The behavior for error rates far below the threshold is also important, as is the number of physical qubits needed to achieve the sought after level of error suppression. Nevertheless, the code threshold provides us with a very easy and useful tool to benchmark different codes and different decoding algorithms, and to compare them with each other. It is common to benchmark codes under the independent and phenomenological noise models of X For this reason, in this thesis we will heavily rely on the value of the code threshold. 

For the toric code, when the only source of errors is from the independent noise model, the optimal threshold has been proven to be X, and including measurement errors X. These values are obtained by matching the pairing problem to the two-dimensional random-bond Ising model [0]. The error threshold is the critical point at which the Nishimori line crosses the boundary between the magnetic and the paramagnetic phase. In order to achieve this value, an optimal decoder needs to consider all possible error configurations on the lattice to identify the correction operator X that is most likely to be equal to the error operator X. This is a computationally heavy task that scales exponentially with the lattice size. It is therefore an impractical approach in reality.

Luckily, there exists other decoding algorithms that can find a solution much faster, albeit at the cost of reducing the code threshold. When surfaces codes were introduced [0], a decoder was simultaneously suggested that scales cubic with the system, which allows for faster decoding. The Minimum-Weight Perfect Matching decoder, which is based on Edmond's Blossom algorithm achieves a code threshold of 10.3\% (Chapter X). Including faulty measurements the threshold drops to 2.9\%. In the recent years, the development of decoders for the surface code has in majority favored the class of neural network decoders, in which in varying methods a certain model is trained to give better predictions on the correction operator [0]. 

A promising and fast decoder is the Union-Find decoder, which is a relatively new addition to the realm of surface code decoders [0]. It scales almost linearly with the system, and has a code threshold of 9.9\% (Chapter X). The former is a major advantage of the Union-Find decoder as the scalability translates directly into the computation time needed to provide the correction. This allows the increase of the system size to obtain enhanced fault-tolerance without dramatically increased computation times. The Minimum-Weight Perfect Matching decoder neural network decoders and other types of decoder all suffer in this regard. In fact, there is no decoder currently available that is fast enough to correct the accumulation of errors with the current physical limitations. Another benefit of the Union-Find decoder over other types of decoders is because it is simple. Even though it may not seem so due to the length of this thesis, the concept of the Union-Find decoder is much more straightforward compared with other, more advanced decoders. 

Quasiparticle picture
The processes of error detection and correction can alternatively be presented in the quasiparticle picture, where the anticommuting stabilizer measurements act like excitations on the lattice, which behave like the quasiparticles anyons. A single error creates a pair of anyons, and a chain of errors causes movement of the anyon on the lattice. A pair of anyons can also annihilate each other when two error chains merge. The correction of errors can thus be viewed of movement of the correction chains until all anyons are annihilated. The quasiparticle picture removes the distracting underlying lattice from the problem, and decoding becomes simply identifying the right pairing between anyons to minimize the chance of a logical error.


Figure Xa shows the quasiparticle representation of the errors suffered in Figure Xa, which has suffered Z (blue lines) and X errors (red lines). The corresponding anyons can either be of the star type (red circle) or plaquette type (blue circle). Figure Xb shows a successful decoding. Note that here not all pairs are correctly identified, but the resulting loop still is in the same class of operators. In Figure Xc the correction has failed as the resulting loop in the correction is in a difference class compared to the error. As the loop still commutes with the stabilizer, no error can be detected, but the encoded qubit has acquired a logical error.


Graph picture
Since it is the structure of the lattice that defines the surface code, such a surface by be also denoted by a graph X. The graph is constructed by X by a vertex set X, edge set X and face set X. Each edge X is defined by a pair of distinct vertices X where X. Each face is a region that has the homology of a disk and is defined by the set of edges on its boundary. 

With respect to the toric code, each vertex refers to an ancillary qubit corresponding to a star operator X, and each face X refers to an ancillary qubit corresponding to a plaquette operator X. The edges are thus the qubits, with two edges per qubit, one spanned by neighboring vertices and another by neighboring faces. Due to the duality of the lattice, the equivalence of vertices with stars and faces with plaquettes can naturally be switched. Also, due to this, the graph G can be split into two separate graphs X and X, corresponding to the primal and dual lattices, where X are the edges connecting the vertices X, and X are the edges connecting the faces X. Each qubit is now represented by a single edge in both graphs and X. If we mention a graph spanned by only vertices and edges X, we refer to the graph X of the primal lattice. 

The planar code
Another variant of the surface code is the planar code, which disposes the periodic boundary conditions of the torus. This allows the qubits to be placed onto a flat 2D surface. For real systems in which the qubits physically interact with each other, this is a huge benefit. Therefore, in this thesis, we will consider both toric and planar variants of the surface code.


Stabilizer generators
There are a few key differences between the planar and toric codes. First, a new type of stabilizer generators define the non-periodic boundary of the lattice, which are referred to as X operators. These X operators have only 3 neighboring qubits and are therefore the tensor product of 3 Pauli operators. The X-plaquette operators lie at the east and west boundaries of the lattice (Figure Xa) and the X-star operators lie at the north and south boundaries of the lattice (Figure Xb). In the middle of the lattice, the bulk of the stabilizer generators still consist of 4 Pauli operators, identical to the ones in the toric code (Figure Xc-d). Note that the stabilizer generators are still defined by equation X and X, but now the relevant faces and vertices contain three neighboring qubits.

Stabilizer violations
A second key difference is that now not all errors will cause two stabilizer violations. In the bulk of the qubits on the lattice, a single error will still cause two neighboring stabilizers to measure -1, or create two anyons. At the boundary however, it now may be the case that an error is only included in one plaquette or star operator. This will also mean the decoding in the quasiparticle picture requires a slightly different approach.

Encoded qubits
Furthermore, we can inspect that a planar surface of dimension L has X physical qubits. We can also find that there are X stabilizer generators. As the boundary is now non-periodic, all generators are now independent, and therefore the number independent generators is X. This means that the planar code encodes X a single logical qubit. The logical X and X operators are pictured in Figure Xe-f.

Dual lattice and graph picture
Other properties of the planar code are very similar to the toric code. The dual lattice also exists for the planar code, for example. But the dual lattice exists at a 90 angle compared to the primal lattice to account for the location of the boundary. The elements of the graph X can now be separated into disjoint subsets due to the different properties of its vertices, edges and faces. The vertex set X can now be divided into; the internal vertex set X consisting of 4-Pauli-operator stabilizers; the boundary vertex set X consisting of 3-Pauli-operator stabilizers; and the open vertex set X consisting of non-stabilizer vertices that span edges on the boundary X that connected to a single stabilizer vertex. Note that vertices of X thus do not correspond to any physical stabilizer measurements. Edges connected to two stabilizer vertices belong to the internal edge set X. The graph for the primal lattice is hence X. Elements of the graph for the dual graph can be separated into disjoint subsets in the same way.


The bulk of the planar code is similar to the toric code, where the stabilizer generators consists of 4 Pauli operators. This is especially true as the system size n increases, as the internal elements scale with n and the boundary elements scale with X. Hence, the decoding algorithms for the planar code is very similar to the toric code, albeit some slight alterations will be needed. 

Faulty measurements

Under the phenomenological noise model X, which includes faulty measurements, the surface code can not be decoded with just a single cycle of stabilizer measurements. As for any measured syndrome we are unsure whether it is caused by a qubit error or measurement error. This can be solved by making multiple rounds of stabilizer measurements in which we gain information about these measurement errors. 

With multiple rounds of measurement, the set of stabilizer measurements is now a three-dimensional array in which the third dimension represents time. For a X lattice, it is common to perform L rounds of measurements, which results in a X array. The space domain of this array represents the measurements taken during a single time-step. In the quasiparticle representation, an anyon is now defined as a measurement that differs from its value in the previous round of measurement. A physical error on a qubit now causes a set of anyons separated in space within the same time-step, whereas a measurement error creates a pair of anyons separated in the time dimension of the array. Decoding is now equivalent to paring anyons in a three-dimensional space. 


Threshold simulations

In the coming chapters, we describe two types of decoders and introduce a new type of decoder that combines elements from the previous two. To benchmark the performances of these decoders, we simulate for the decoder success rate X on toric and planar lattices for the independent noise model (Definition X), and on lattices for the phenomenological noise model (Definition X). Under these error models, the lattice can be separated into the primal lattice containing the star operator X-parity check measurement, and the dual lattice containing the plaquette operator Z-parity check measurements. Any simulation will be performed on the primal lattice with a single error rate X for both bit-flips and noisy measurements. The dual lattice is under these conditions equivalent to the primal lattice. 

To find the code threshold X, we will perform Monte Carlo simulations of a lattice with some random error according to the input error model and the decoder process. The number of successful corrections divided by the number of simulations is a measure for the decoder success rate X. We will simulate for a range of lattices sizes and error rates around the suspected threshold value. We will then fit the acquired data to the function
for X [0]. Here, all but X, p and L are fitting parameters. The threshold error rate X, as well as the decoding success rate at the threshold X and the running times will be used to benchmark and compare decoders. 

The Monte Carlo simulations are performed on our own implementation in Python3 (see Appendix X). All thresholds are pre-computed on a 3.20Ghz Intel Core i5-4460 CPU with 16 Gigabytes of memory. The range of error rates is then refined around this threshold value and passed to a compute-cluster for rapid Monte Carlo simulations. The compute-cluster used in this thesis is the Cartesius system belonging the SURF cooperative in Amsterdam, The Netherlands, which has 3.60Ghz Intel Xeon E5 CPU's with 64 Gigabytes of memory. On every CPU we maximize parallelization by running 24 threads of individual Monte Carlo simulations. 

